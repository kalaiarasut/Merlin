{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db8d966",
   "metadata": {},
   "source": [
    "## 1. Check GPU & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ed7f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c162cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional dependencies\n",
    "!pip install -q albumentations timm\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2058f81e",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive & Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úÖ Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ‚ö†Ô∏è CONFIGURE YOUR CSV PATH HERE\n",
    "CSV_PATH = \"/content/drive/MyDrive/otolith_species.csv\"  # <-- CHANGE THIS IF NEEDED\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"‚úÖ Loaded {len(df)} records\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nUnique species: {df['scientificName'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb1884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data - remove rows with missing URLs\n",
    "df = df[df['associatedMedia'].notna()].copy()\n",
    "df = df[df['associatedMedia'].astype(str).str.startswith('http')].copy()\n",
    "print(f\"Records with valid URLs: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d7804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze species distribution\n",
    "species_counts = df['scientificName'].value_counts()\n",
    "print(\"Top 20 species by image count:\")\n",
    "print(species_counts.head(20))\n",
    "\n",
    "print(f\"\\n\\nSpecies with >= 50 images: {(species_counts >= 50).sum()}\")\n",
    "print(f\"Species with >= 20 images: {(species_counts >= 20).sum()}\")\n",
    "print(f\"Species with >= 10 images: {(species_counts >= 10).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04563228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot species distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "species_counts.head(30).plot(kind='bar')\n",
    "plt.title('Top 30 Species by Image Count')\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17a0f3a",
   "metadata": {},
   "source": [
    "## 3. Filter & Prepare Dataset\n",
    "\n",
    "We'll filter to species with enough samples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ‚ö†Ô∏è CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "MIN_SAMPLES_PER_SPECIES = 10  # Minimum images per species to include\n",
    "MAX_SPECIES = 50  # Maximum number of species to classify (top N by count)\n",
    "\n",
    "# Filter species with enough samples\n",
    "valid_species = species_counts[species_counts >= MIN_SAMPLES_PER_SPECIES].head(MAX_SPECIES).index.tolist()\n",
    "df_filtered = df[df['scientificName'].isin(valid_species)].copy()\n",
    "\n",
    "print(f\"Selected {len(valid_species)} species with >= {MIN_SAMPLES_PER_SPECIES} samples\")\n",
    "print(f\"Total images for training: {len(df_filtered)}\")\n",
    "\n",
    "# Create label encoding\n",
    "species_to_idx = {species: idx for idx, species in enumerate(sorted(valid_species))}\n",
    "idx_to_species = {idx: species for species, idx in species_to_idx.items()}\n",
    "\n",
    "df_filtered['label'] = df_filtered['scientificName'].map(species_to_idx)\n",
    "\n",
    "print(f\"\\nNumber of classes: {len(species_to_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f14316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save label mapping for later use\n",
    "import json\n",
    "\n",
    "label_mapping = {\n",
    "    'species_to_idx': species_to_idx,\n",
    "    'idx_to_species': idx_to_species,\n",
    "    'num_classes': len(species_to_idx)\n",
    "}\n",
    "\n",
    "print(\"Label mapping (first 10):\")\n",
    "for species, idx in list(species_to_idx.items())[:10]:\n",
    "    print(f\"  {idx}: {species}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8892bd12",
   "metadata": {},
   "source": [
    "## 4. Download Otolith Images\n",
    "\n",
    "Download images from the AFORO server. This may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6669a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Create directories\n",
    "IMAGE_DIR = Path(\"/content/otolith_images\")\n",
    "IMAGE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def download_image(row):\n",
    "    \"\"\"Download a single image with error handling\"\"\"\n",
    "    url = row.get('associatedMedia')\n",
    "    \n",
    "    # Skip if URL is missing or invalid\n",
    "    if not url or not isinstance(url, str) or not url.startswith('http'):\n",
    "        return False, \"Invalid URL\"\n",
    "    \n",
    "    catalog_num = str(row['catalogNumber'])\n",
    "    label = row['label']\n",
    "    \n",
    "    # Create species folder\n",
    "    species_folder = IMAGE_DIR / f\"class_{label:03d}\"\n",
    "    species_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Filename\n",
    "    try:\n",
    "        ext = url.split('.')[-1].lower()\n",
    "        if ext not in ['jpg', 'jpeg', 'png', 'tif', 'tiff']:\n",
    "            ext = 'tif'\n",
    "    except:\n",
    "        ext = 'tif'\n",
    "    \n",
    "    filename = species_folder / f\"{catalog_num}.{ext}\"\n",
    "    \n",
    "    # Skip if exists\n",
    "    if filename.exists():\n",
    "        return True, str(filename)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            return True, str(filename)\n",
    "        else:\n",
    "            return False, f\"HTTP {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "print(f\"Will download {len(df_filtered)} images...\")\n",
    "print(\"This may take 10-20 minutes.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c419e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images with progress bar\n",
    "successful = 0\n",
    "failed = 0\n",
    "failed_urls = []\n",
    "\n",
    "# Convert to list of dicts for easier processing\n",
    "records = df_filtered.to_dict('records')\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = {executor.submit(download_image, row): row for row in records}\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Downloading\"):\n",
    "        success, result = future.result()\n",
    "        if success:\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "            if failed <= 10:\n",
    "                failed_urls.append(result)\n",
    "\n",
    "print(f\"\\n‚úÖ Downloaded: {successful}\")\n",
    "print(f\"‚ùå Failed: {failed}\")\n",
    "\n",
    "if failed_urls:\n",
    "    print(f\"\\nSample failures: {failed_urls[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify downloaded images\n",
    "from PIL import Image\n",
    "\n",
    "# Count images per class\n",
    "class_counts = {}\n",
    "all_images = []\n",
    "\n",
    "for class_folder in sorted(IMAGE_DIR.iterdir()):\n",
    "    if class_folder.is_dir():\n",
    "        images = list(class_folder.glob(\"*\"))\n",
    "        class_idx = int(class_folder.name.split('_')[1])\n",
    "        class_counts[class_idx] = len(images)\n",
    "        all_images.extend([(str(img), class_idx) for img in images])\n",
    "\n",
    "print(f\"Total images downloaded: {len(all_images)}\")\n",
    "print(f\"Classes with images: {len(class_counts)}\")\n",
    "print(f\"\\nImages per class (first 10):\")\n",
    "for idx in sorted(class_counts.keys())[:10]:\n",
    "    species_name = idx_to_species.get(idx, 'Unknown')[:40]\n",
    "    print(f\"  Class {idx} ({species_name}...): {class_counts[idx]} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6434e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "import random\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "sample_images = random.sample(all_images, min(12, len(all_images)))\n",
    "\n",
    "for idx, (img_path, label) in enumerate(sample_images):\n",
    "    ax = axes[idx // 4, idx % 4]\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        ax.imshow(img)\n",
    "        species_name = idx_to_species.get(label, 'Unknown')\n",
    "        short_name = ' '.join(species_name.split()[:2])\n",
    "        ax.set_title(f\"{short_name}\\n(Class {label})\", fontsize=9)\n",
    "    except Exception as e:\n",
    "        ax.set_title(f\"Error loading\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample Otolith Images by Species', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d794fe1",
   "metadata": {},
   "source": [
    "## 5. Create Dataset & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb19b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "class OtolithDataset(Dataset):\n",
    "    \"\"\"Dataset for otolith species classification\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels, transform=None, image_size=224):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform or self._default_transform()\n",
    "    \n",
    "    def _default_transform(self):\n",
    "        return A.Compose([\n",
    "            A.Resize(self.image_size, self.image_size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert('RGB'))\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed['image']\n",
    "        except Exception as e:\n",
    "            image = torch.zeros(3, self.image_size, self.image_size)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "print(\"‚úÖ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfd449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare image paths and labels\n",
    "image_paths = [x[0] for x in all_images]\n",
    "labels = [x[1] for x in all_images]\n",
    "\n",
    "# Filter out classes with too few samples\n",
    "class_counts_check = Counter(labels)\n",
    "print(f\"Total classes before filtering: {len(class_counts_check)}\")\n",
    "\n",
    "# Keep only classes with at least 3 images\n",
    "MIN_IMAGES = 3\n",
    "valid_classes = {cls for cls, count in class_counts_check.items() if count >= MIN_IMAGES}\n",
    "print(f\"Classes with >= {MIN_IMAGES} images: {len(valid_classes)}\")\n",
    "\n",
    "# Filter\n",
    "filtered_data = [(path, label) for path, label in zip(image_paths, labels) if label in valid_classes]\n",
    "image_paths = [x[0] for x in filtered_data]\n",
    "labels = [x[1] for x in filtered_data]\n",
    "\n",
    "print(f\"Total images after filtering: {len(image_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1eb4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/val/test (without stratify to avoid errors)\n",
    "# First split: 80% train, 20% temp\n",
    "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 50% val, 50% test\n",
    "val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
    "    temp_paths, temp_labels, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_paths)} images\")\n",
    "print(f\"Val: {len(val_paths)} images\")\n",
    "print(f\"Test: {len(test_paths)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1976cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(var_limit=(10, 50)),\n",
    "        A.GaussianBlur(blur_limit=3),\n",
    "    ], p=0.3),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = OtolithDataset(train_paths, train_labels, train_transform, IMAGE_SIZE)\n",
    "val_dataset = OtolithDataset(val_paths, val_labels, val_transform, IMAGE_SIZE)\n",
    "test_dataset = OtolithDataset(test_paths, test_labels, val_transform, IMAGE_SIZE)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45d0de",
   "metadata": {},
   "source": [
    "## 6. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfaa4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "class OtolithSpeciesClassifier(nn.Module):\n",
    "    \"\"\"CNN for otolith species classification using transfer learning\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, model_name='efficientnet_b0', pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained backbone\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0)\n",
    "        \n",
    "        # Get feature dimension\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 224, 224)\n",
    "            features = self.backbone(dummy)\n",
    "            self.feature_dim = features.shape[1]\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Feature dimension: {self.feature_dim}\")\n",
    "        print(f\"Number of classes: {num_classes}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "# Get number of unique classes from actual data\n",
    "NUM_CLASSES = len(set(labels))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = OtolithSpeciesClassifier(NUM_CLASSES, model_name='efficientnet_b0')\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02152023",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f92eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Training configuration\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.0001\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LEARNING_RATE/100)\n",
    "\n",
    "print(f\"Training for {EPOCHS} epochs\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16066d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "from datetime import datetime\n",
    "\n",
    "OUTPUT_DIR = Path(\"/content/drive/MyDrive/otolith_models/species_classifier\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_dir = OUTPUT_DIR / f\"run_{timestamp}\"\n",
    "run_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Saving to: {run_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1638f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': []\n",
    "}\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "early_stop_patience = 10\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Record history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"  LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'num_classes': NUM_CLASSES,\n",
    "            'species_to_idx': species_to_idx,\n",
    "            'idx_to_species': idx_to_species,\n",
    "        }, run_dir / \"checkpoint_best.pt\")\n",
    "        print(f\"  ‚úÖ New best model saved! (Acc: {val_acc:.2f}%)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"\\n‚ö†Ô∏è Early stopping after {epoch + 1} epochs\")\n",
    "            break\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(f\"   Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"   Model saved to: {run_dir}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ba8f51",
   "metadata": {},
   "source": [
    "## 8. Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da966383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train')\n",
    "axes[1].plot(history['val_acc'], label='Validation')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(run_dir / \"training_curves.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a29a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate on test set\n",
    "checkpoint = torch.load(run_dir / \"checkpoint_best.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "test_loss, test_acc = validate(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"\\nüìä Test Set Results:\")\n",
    "print(f\"   Loss: {test_loss:.4f}\")\n",
    "print(f\"   Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd23f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels_batch in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels_batch.numpy())\n",
    "\n",
    "# Get unique classes in test set\n",
    "unique_classes = sorted(set(all_labels))\n",
    "print(f\"Classes in test set: {len(unique_classes)}\")\n",
    "\n",
    "# Classification report for top classes\n",
    "top_classes = unique_classes[:10]\n",
    "print(\"\\nClassification Report (Top 10 classes):\")\n",
    "print(classification_report(\n",
    "    all_labels, all_preds, \n",
    "    labels=top_classes,\n",
    "    target_names=[idx_to_species.get(i, f'Class {i}')[:30] for i in top_classes],\n",
    "    zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e478ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for top classes\n",
    "TOP_N = min(15, len(unique_classes))\n",
    "top_classes = unique_classes[:TOP_N]\n",
    "\n",
    "# Filter to top classes\n",
    "mask = [l in top_classes for l in all_labels]\n",
    "filtered_labels = [l for l, m in zip(all_labels, mask) if m]\n",
    "filtered_preds = [p for p, m in zip(all_preds, mask) if m]\n",
    "\n",
    "if len(filtered_labels) > 0:\n",
    "    cm = confusion_matrix(filtered_labels, filtered_preds, labels=top_classes)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=[idx_to_species.get(i, '?').split()[0][:10] for i in top_classes],\n",
    "                yticklabels=[idx_to_species.get(i, '?').split()[0][:10] for i in top_classes])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix (Top {TOP_N} Species)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(run_dir / \"confusion_matrix.png\", dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough data for confusion matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8757a5",
   "metadata": {},
   "source": [
    "## 9. Export Model for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581bdd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX format\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE).to(device)\n",
    "\n",
    "onnx_path = run_dir / \"species_classifier.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    input_names=['image'],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes={'image': {0: 'batch_size'}, 'logits': {0: 'batch_size'}}\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ ONNX model exported: {onnx_path}\")\n",
    "print(f\"   File size: {os.path.getsize(onnx_path) / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554af5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete model info\n",
    "model_info = {\n",
    "    'model_name': 'efficientnet_b0',\n",
    "    'image_size': IMAGE_SIZE,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'species_to_idx': species_to_idx,\n",
    "    'idx_to_species': {str(k): v for k, v in idx_to_species.items()},\n",
    "    'best_val_acc': best_val_acc,\n",
    "    'test_acc': test_acc,\n",
    "    'train_samples': len(train_paths),\n",
    "    'val_samples': len(val_paths),\n",
    "    'test_samples': len(test_paths),\n",
    "    'timestamp': timestamp,\n",
    "}\n",
    "\n",
    "with open(run_dir / \"model_info.json\", \"w\") as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(\"\\nüìÅ Saved files:\")\n",
    "for f in sorted(run_dir.iterdir()):\n",
    "    size = os.path.getsize(f) / 1e6\n",
    "    print(f\"   {f.name} ({size:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad436b",
   "metadata": {},
   "source": [
    "## 10. Test Single Image Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e586fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_species(model, image_path, device, idx_to_species, top_k=5):\n",
    "    \"\"\"Predict species from an otolith image\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess\n",
    "    image = np.array(Image.open(image_path).convert('RGB'))\n",
    "    \n",
    "    transform = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    transformed = transform(image=image)\n",
    "    image_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probs = torch.softmax(outputs, dim=1)[0]\n",
    "        top_probs, top_indices = probs.topk(top_k)\n",
    "    \n",
    "    results = []\n",
    "    for prob, idx in zip(top_probs.cpu().numpy(), top_indices.cpu().numpy()):\n",
    "        results.append({\n",
    "            'species': idx_to_species.get(idx, f'Unknown class {idx}'),\n",
    "            'confidence': float(prob)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test on a random image\n",
    "if len(test_paths) > 0:\n",
    "    test_image = random.choice(test_paths)\n",
    "    predictions = predict_species(model, test_image, device, idx_to_species)\n",
    "    \n",
    "    print(\"\\nüîç Sample Prediction:\")\n",
    "    print(f\"Image: {Path(test_image).name}\")\n",
    "    print(f\"\\nTop 5 Predictions:\")\n",
    "    for i, pred in enumerate(predictions, 1):\n",
    "        print(f\"  {i}. {pred['species'][:50]}\")\n",
    "        print(f\"     Confidence: {pred['confidence']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction\n",
    "if len(test_paths) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Show image\n",
    "    img = Image.open(test_image).convert('RGB')\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(f\"Input Image\\n{Path(test_image).name}\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Show predictions\n",
    "    species_names = [p['species'].split('(')[0].strip()[:25] for p in predictions]\n",
    "    confidences = [p['confidence'] for p in predictions]\n",
    "    colors = ['green' if i == 0 else 'steelblue' for i in range(len(predictions))]\n",
    "    \n",
    "    axes[1].barh(species_names[::-1], confidences[::-1], color=colors[::-1])\n",
    "    axes[1].set_xlabel('Confidence')\n",
    "    axes[1].set_title('Species Predictions')\n",
    "    axes[1].set_xlim(0, 1)\n",
    "    \n",
    "    for i, (name, conf) in enumerate(zip(species_names[::-1], confidences[::-1])):\n",
    "        axes[1].text(conf + 0.02, i, f'{conf:.1%}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9624c61",
   "metadata": {},
   "source": [
    "## üéâ Done!\n",
    "\n",
    "Your trained species classifier is saved in Google Drive:\n",
    "- `checkpoint_best.pt` - PyTorch model with label mappings\n",
    "- `species_classifier.onnx` - ONNX format for production\n",
    "- `model_info.json` - Model metadata and species list\n",
    "\n",
    "### Next Steps:\n",
    "1. Download the model files from Google Drive\n",
    "2. Copy to your project: `ai-services/models/`\n",
    "3. Integrate with your Ocean platform\n",
    "\n",
    "### Integration Example:\n",
    "```python\n",
    "# In your otolith_analyzer.py\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Load model\n",
    "checkpoint = torch.load('models/species_classifier/checkpoint_best.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "idx_to_species = checkpoint['idx_to_species']\n",
    "\n",
    "# Predict\n",
    "species = predict_species(model, image_path, device, idx_to_species)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
